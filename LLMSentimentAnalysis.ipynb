{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac014ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import spacy\n",
    "import random\n",
    "import logging\n",
    "import pytextrank\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb154a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the input to how many tokens, default to 512 for BERT use.\n",
    "# Since LongFormer could accept 4096 tokens, we could skip TextRank if LongFormer\n",
    "seed_val = 42\n",
    "ENABLE_TEXT_RANK = False\n",
    "TEXT_RANK_LENGTH = 512\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "MODEL = [\"T5base\",\"roberta\",\"Distilbert\",\"FlanT5small\",\"FlanT5base\"] # Choose from \"T5base\", \"roberta\", \"Distilbert\", \"FlanT5small\"\n",
    "\n",
    "NEW_BATCH_SIZE = 8 if MODEL in [\"roberta\", \"Distilbert\", \"FlanT5small\"] else 4 \n",
    "\n",
    "#wandb.login(key='bf24a38a046a0448057459477a5d48fbc6eb2f6a')\n",
    "#wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea032a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "# If GPU not available, training will cost SEVERAL DAYS, not recommended running on CPU\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU insteadp(not recommended).')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6ca01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/fangkangmi/.cache/huggingface/datasets/argilla___parquet/argilla--banking_sentiment_setfit-4a60f83f113675bf/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6330494660704037b158b088d7cfba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['are you sending me my card?',\n",
       "  \"Two days ago I did a transfer to another account within the country.  It doesn't appear the transfer went through.  I have verified the account number several times.  Could you please check on this for me?\",\n",
       "  \"Why didn't I receive the right amount of cash?\",\n",
       "  \"Is there a reason why my virtual card won't work?\",\n",
       "  'Why is my balance the same after a transfer?'],\n",
       " 'label': [1, 1, 0, 0, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"argilla/banking_sentiment_setfit\")\n",
    "test = concatenate_datasets([dataset['train'], dataset['test']])\n",
    "test_text = test['text']\n",
    "test_label = ['neutral' if i == 1 else 'negative' for i in test['label']]\n",
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e202a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba67bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = {\n",
    "    \"T5base\": \"michelecafagna26/t5-base-finetuned-sst2-sentiment\",\n",
    "    \"roberta\":\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"Distilbert\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"FlanT5small\": \"cardiffnlp/flan-t5-small-tweet-sentiment\",\n",
    "    \"FlanT5base\": \"cardiffnlp/flan-t5-base-tweet-sentiment\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109e4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_and_tokenizer(model):\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_options.get(model)) if model in [\"T5base\", \"FlanT5small\", \"FlanT5base\"] \\\n",
    "           else AutoModelForSequenceClassification.from_pretrained(model_options.get(model))\\\n",
    "           ,AutoTokenizer.from_pretrained(model_options.get(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c0838",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "By now the test dataset, model and tokenizer has been loaded. The next step is to tokenize the dataset and evaluate their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c3d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31458/1883409789.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  T5_predict = T5_get_sentiment(torch.tensor(T5_tokenized_text))\n",
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.94      0.54        51\n",
      "     neutral       0.81      0.14      0.24        93\n",
      "\n",
      "    accuracy                           0.42       144\n",
      "   macro avg       0.59      0.54      0.39       144\n",
      "weighted avg       0.66      0.42      0.34       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def T5_get_sentiment(tensor):\n",
    "    preds = model.generate(tensor)\n",
    "    decoded_preds = tokenizer.batch_decode(sequences=preds, skip_special_tokens=True)\n",
    "    return decoded_preds\n",
    "\n",
    "if('T5base' in MODEL):\n",
    "    model, tokenizer = set_model_and_tokenizer('T5base')\n",
    "    \n",
    "    #Add prompt\n",
    "    T5_test_text = [\"sentiment: \" + item for item in test_text]\n",
    "    #Tokenizer\n",
    "    T5_tokenized_text = tokenizer(T5_test_text, max_length=128, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "    \n",
    "    #Create the prediction\n",
    "    T5_predict = T5_get_sentiment(torch.tensor(T5_tokenized_text))\n",
    "    T5_predict_digit = ['neutral' if i == 'p' else 'negative' for i in T5_predict]\n",
    "    \n",
    "    # Print the classification report\n",
    "    # 1 means positive 0 means negative\n",
    "    report = classification_report(test_label,T5_predict_digit)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a4bcaa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31458/4160514151.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.73      0.70        51\n",
      "     neutral       0.84      0.81      0.82        93\n",
      "\n",
      "    accuracy                           0.78       144\n",
      "   macro avg       0.76      0.77      0.76       144\n",
      "weighted avg       0.78      0.78      0.78       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RoBERTa_Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "if('roberta' in MODEL):\n",
    "\n",
    "    model, tokenizer = set_model_and_tokenizer('roberta')\n",
    "    \n",
    "    #Add prompt\n",
    "    RoBERTa_test_text = [\" \" + item for item in test_text]\n",
    "    \n",
    "    #Tokenize, with max_length= 128 could increase the accuracy from 50 -> 53\n",
    "    RoBERTa_tokenized_text = tokenizer(RoBERTa_test_text,max_length=128, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    #Dataset and Dataloader\n",
    "    RoBERTa_dataset_test = RoBERTa_Dataset(RoBERTa_tokenized_text,test['label'])\n",
    "    RoBERTa_dataloader = DataLoader(RoBERTa_dataset_test, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in RoBERTa_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Perform inference\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predicted_values = outputs.logits\n",
    "\n",
    "        predictions.extend(predicted_values)\n",
    "        true_labels.extend(batch['labels'].tolist())\n",
    "    # Convert logits to predictions\n",
    "    # 1 and 2 means positive and 0 means negative\n",
    "    predictions = [torch.argmax(item).item() for item in predictions]\n",
    "    predictions = ['neutral' if (i == 1 or i == 2) else 'negative' for i in predictions]\n",
    "    true_labels = ['neutral' if (i == 1 or i == 2) else 'negative' for i in true_labels]\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617566d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      1.00      0.52        12\n",
      "     neutral       1.00      0.08      0.15        24\n",
      "\n",
      "    accuracy                           0.39        36\n",
      "   macro avg       0.68      0.54      0.34        36\n",
      "weighted avg       0.78      0.39      0.28        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32008/702547249.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the test dataset\n",
    "model, tokenizer = set_model_and_tokenizer('Distilbert')\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in Distilbert_dataloader_test:\n",
    "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Perform inference\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    predicted_values = outputs.logits\n",
    "    predictions.extend(predicted_values)\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "# Convert logits to predictions,  \n",
    "# 1 means positive and 0 means negative\n",
    "predictions = [torch.argmax(item).item() for item in predictions]\n",
    "predictions = ['neutral' if i == 1 else 'negative' for i in predictions]\n",
    "true_labels = ['neutral' if i == 1 else 'negative' for i in true_labels]\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predictions)\n",
    "print(report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ba6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31458/2342649218.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 2.6344, -2.1943], device='cuda:0'), tensor([ 4.2249, -3.4419], device='cuda:0'), tensor([ 4.1324, -3.3406], device='cuda:0'), tensor([ 3.8986, -3.2051], device='cuda:0'), tensor([ 4.2655, -3.4120], device='cuda:0')]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.94      0.51        51\n",
      "     neutral       0.57      0.04      0.08        93\n",
      "\n",
      "    accuracy                           0.36       144\n",
      "   macro avg       0.46      0.49      0.30       144\n",
      "weighted avg       0.49      0.36      0.23       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Distilbert_Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "if('Distilbert' in MODEL):\n",
    "    model, tokenizer = set_model_and_tokenizer('Distilbert')\n",
    "    \n",
    "    #No prompt for this \n",
    "    Distilbert_tokenized_text = tokenizer(RoBERTa_test_text,max_length=128, padding=True,  return_tensors=\"pt\")\n",
    "    \n",
    "    #Dataset and Dataloader\n",
    "    Distilbert_dataset_test = Distilbert_Dataset(Distilbert_tokenized_text,test['label'])\n",
    "    Distilbert_dataloader = DataLoader(Distilbert_dataset_test, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in Distilbert_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Perform inference\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predicted_values = outputs.logits\n",
    "        predictions.extend(predicted_values)\n",
    "        true_labels.extend(batch['labels'].tolist())\n",
    "    # Convert logits to predictions,  \n",
    "    # 1 means positive and 0 means negative\n",
    "    predictions = [torch.argmax(item).item() for item in predictions]\n",
    "    predictions = ['neutral' if i == 1 else 'negative' for i in predictions]\n",
    "    true_labels = ['neutral' if i == 1 else 'negative' for i in true_labels]\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c3c1fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31458/2028437801.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Flan_T5_small_predict = Flan_T5_get_sentiment(torch.tensor(Flan_T5_small_tokenized_test))\n",
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.82      0.63        51\n",
      "     neutral       0.85      0.56      0.68        93\n",
      "\n",
      "    accuracy                           0.65       144\n",
      "   macro avg       0.68      0.69      0.65       144\n",
      "weighted avg       0.73      0.65      0.66       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Flan_T5_get_sentiment(tensor):\n",
    "    preds = model.generate(tensor)\n",
    "    decoded_preds = tokenizer.batch_decode(sequences=preds, skip_special_tokens=True)\n",
    "    return decoded_preds\n",
    "\n",
    "if('FlanT5small' in MODEL):\n",
    "    model, tokenizer = set_model_and_tokenizer('FlanT5small')\n",
    "    \n",
    "    #Add prompt\n",
    "    Flan_T5_small_test_text = [\"Bank sentiment: \"+ item for item in test_text]\n",
    "    #Tokenizer\n",
    "    Flan_T5_small_tokenized_test = tokenizer(Flan_T5_small_test_text, \\\n",
    "                                             max_length=128, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "    \n",
    "    \n",
    "    Flan_T5_small_predict = Flan_T5_get_sentiment(torch.tensor(Flan_T5_small_tokenized_test))\n",
    "    Flan_T5_small_predict_digit = ['neutral' if ('neutral' in i or 'positive' in i) else 'negative' for i in Flan_T5_small_predict]\n",
    "    # Print the classification report\n",
    "    # 1 means positive 0 means negative\n",
    "    report = classification_report(test_label,Flan_T5_small_predict_digit)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac30ebb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " '(ii).',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'positive',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral',\n",
       " 'negative or neutral']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flan_T5_small_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398a7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31458/587979243.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Flan_T5_base_predict = Flan_T5_get_sentiment(torch.tensor(Flan_T5_base_tokenized_test))\n",
      "/home/fangkangmi/miniconda3/envs/textsum/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.98      0.56        51\n",
      "     neutral       0.94      0.17      0.29        93\n",
      "\n",
      "    accuracy                           0.46       144\n",
      "   macro avg       0.67      0.58      0.43       144\n",
      "weighted avg       0.75      0.46      0.39       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Flan_T5_get_sentiment(tensor):\n",
    "    preds = model.generate(tensor)\n",
    "    decoded_preds = tokenizer.batch_decode(sequences=preds, skip_special_tokens=True)\n",
    "    return decoded_preds\n",
    "\n",
    "if('FlanT5base' in MODEL):\n",
    "    model, tokenizer = set_model_and_tokenizer('FlanT5base')\n",
    "    \n",
    "    #Add prompt\n",
    "    Flan_T5_base_test_text = [\"Please classify the sentiment of follwing text as 'neutral', 'positive' or 'negative': \"\\\n",
    "                              + item for item in test_text]\n",
    "    #Tokenizer\n",
    "    Flan_T5_base_tokenized_test = tokenizer(Flan_T5_base_test_text, \\\n",
    "                                             max_length=128, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "    \n",
    "    \n",
    "    Flan_T5_base_predict = Flan_T5_get_sentiment(torch.tensor(Flan_T5_base_tokenized_test))\n",
    "    Flan_T5_base_predict_digit = ['negative' if i == 'negative' else 'neutral' for i in Flan_T5_base_predict]\n",
    "    # Print the classification report\n",
    "    # 1 means positive 0 means negative\n",
    "    report = classification_report(test_label,Flan_T5_base_predict_digit)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc4707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'neutral',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flan_T5_base_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c565a7",
   "metadata": {},
   "source": [
    "Below are training DistilBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"argilla/banking_sentiment_setfit\")\n",
    "DistilBERT_train = dataset['train']\n",
    "DistilBERT_test = dataset['test']\n",
    "\n",
    "DistilBERT_train_text, DistilBERT_test_text = DistilBERT_train['text'], DistilBERT_test['text']\n",
    "\n",
    "DistilBERT_train_label = ['neutral' if i == 1 else 'negative' for i in test['label']]\n",
    "DistilBERT_test_label = ['neutral' if i == 1 else 'negative' for i in test['label']]\n",
    "\n",
    "\n",
    "class Distilbert_Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "model, tokenizer = set_model_and_tokenizer('Distilbert')\n",
    "\n",
    "#No prompt for this \n",
    "Distilbert_train_tokenized_text = tokenizer(DistilBERT_train_text,max_length=128, padding=True,  return_tensors=\"pt\")\n",
    "Distilbert_test_tokenized_text = tokenizer(DistilBERT_test_text,max_length=128, padding=True,  return_tensors=\"pt\")\n",
    "\n",
    "#Dataset and Dataloader\n",
    "Distilbert_dataset_train = Distilbert_Dataset(Distilbert_train_tokenized_text,DistilBERT_train['label'])\n",
    "Distilbert_dataset_test = Distilbert_Dataset(Distilbert_test_tokenized_text,DistilBERT_test['label'])\n",
    "Distilbert_dataloader_train = DataLoader(Distilbert_dataset_train, batch_size=8, shuffle=True)\n",
    "Distilbert_dataloader_test = DataLoader(Distilbert_dataset_test, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "losses = []\n",
    "start_epoch = 0\n",
    "\n",
    "# Training \n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "    for batch in tqdm(Distilbert_dataloader, desc=\"Training\"):\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Append the loss value to the list\n",
    "        losses.append(loss.item())  \n",
    "\n",
    "    # Plot the loss values after each epoch\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "# Iterate over the test dataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in Distilbert_dataloader_test:\n",
    "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Perform inference\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    predicted_values = outputs.logits\n",
    "    predictions.extend(predicted_values)\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "# Convert logits to predictions,  \n",
    "# 1 means positive and 0 means negative\n",
    "predictions = [torch.argmax(item).item() for item in predictions]\n",
    "predictions = ['neutral' if i == 1 else 'negative' for i in predictions]\n",
    "true_labels = ['neutral' if i == 1 else 'negative' for i in true_labels]\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predictions)\n",
    "print(report)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b25261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum",
   "language": "python",
   "name": "textsum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
